import streamlit as st
import json
import os
import faiss
import numpy as np
import re
from sentence_transformers import SentenceTransformer

# é¡µé¢é…ç½®
st.set_page_config(page_title="åŒ»ç–—é—®ç­”åŠ©æ‰‹", page_icon="ğŸ’Š")

# æ–‡æœ¬æ ‡å‡†åŒ–å¤„ç†ï¼ˆæ¸…é™¤å¤šä½™ç¬¦å·ã€ç©ºæ ¼ï¼‰
def clean_text(text):
    text = text.strip()
    text = re.sub(r"[ï¼Œ,]", ",", text)
    text = re.sub(r"[ï¼Ÿ?]", "?", text)
    text = re.sub(r"[ï¼!]", "!", text)
    text = re.sub(r"\s+", "", text)  # åˆ é™¤æ‰€æœ‰ç©ºæ ¼
    return text

# åŠ è½½æ¨¡å‹
@st.cache_resource
def load_model():
    return SentenceTransformer("shibing624/text2vec-base-chinese")

model = load_model()

# çŸ¥è¯†åº“è·¯å¾„
KB_PATH = "data/knowledge_base.json"

# åŠ è½½çŸ¥è¯†åº“
def load_knowledge():
    if not os.path.exists(KB_PATH):
        return []
    with open(KB_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

# ä¿å­˜çŸ¥è¯†åº“
def save_knowledge(kb):
    with open(KB_PATH, "w", encoding="utf-8") as f:
        json.dump(kb, f, ensure_ascii=False, indent=2)

# æ„å»ºå‘é‡ç´¢å¼•
def build_index(kb):
    if not kb:
        return None, []
    cleaned_questions = [clean_text(item["question"]) for item in kb]
    embeddings = model.encode(cleaned_questions, convert_to_numpy=True)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index, embeddings

# åŠ è½½çŸ¥è¯†åº“å¹¶æ„å»ºç´¢å¼•
knowledge_base = load_knowledge()
index, embeddings = build_index(knowledge_base)

# æŸ¥è¯¢æœ€ç›¸ä¼¼é—®é¢˜
def search_answer(query, top_k=1):
    if index is None or not knowledge_base:
        return None, 0.0
    cleaned_query = clean_text(query)
    query_vec = model.encode([cleaned_query], convert_to_numpy=True)
    D, I = index.search(query_vec, top_k)
    if D[0][0] < 1.2:  # è®¾ç½®å®¹å¿åº¦ï¼ˆè·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼‰
        return knowledge_base[I[0][0]], D[0][0]
    return None, 0.0

# ä¸»ç•Œé¢
tab1, tab2 = st.tabs(["ğŸ¤– é—®ç­”åŠ©æ‰‹", "ğŸ“š ä¸“å®¶çŸ¥è¯†å½•å…¥"])

with tab1:
    st.title("ğŸ§  åŒ»ç–—é—®ç­”åŠ©æ‰‹")
    st.markdown("è¾“å…¥æ‚¨çš„å¥åº·é—®é¢˜ï¼Œæˆ‘ä»¬å°†æ ¹æ®çŸ¥è¯†åº“æä¾›å‚è€ƒå»ºè®®ã€‚")

    query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", key="query_input")

    if st.button("ğŸ” æŸ¥è¯¢"):
        if not query.strip():
            st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ã€‚")
        else:
            result, score = search_answer(query)
            if result:
                st.success("æ‰¾åˆ°ç›¸å…³ç­”æ¡ˆï¼š")
                st.markdown(f"**Qï¼š** {result['question']}")
                st.markdown(f"**Aï¼š** {result['answer']}")
                if result.get("tags"):
                    st.caption(f"æ ‡ç­¾ï¼š{'ã€'.join(result['tags'])}")
                st.caption(f"ç›¸ä¼¼åº¦ï¼š{1.0 - score:.2f}")
            else:
                st.error("å¾ˆæŠ±æ­‰ï¼Œæˆ‘è¿˜æ²¡æœ‰ç›¸å…³çŸ¥è¯†ã€‚æ¬¢è¿æ·»åŠ ï¼")

with tab2:
    st.subheader("ğŸ“š æ·»åŠ ä¸“å®¶çŸ¥è¯†")
    new_q = st.text_input("é—®é¢˜ï¼š")
    new_a = st.text_area("ç­”æ¡ˆï¼š")
    new_tags = st.text_input("æ ‡ç­¾ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰")

    if st.button("ğŸ“¥ æ·»åŠ æ–°æ¡ç›®"):
        if new_q and new_a:
            new_entry = {
                "question": new_q.strip(),
                "answer": new_a.strip(),
                "tags": [t.strip() for t in new_tags.split(",") if t.strip()]
            }
            knowledge_base.append(new_entry)
            save_knowledge(knowledge_base)
            index, embeddings = build_index(knowledge_base)
            st.success("âœ… æ–°çŸ¥è¯†å·²æ·»åŠ ï¼")
        else:
            st.warning("è¯·å¡«å†™å®Œæ•´çš„é—®é¢˜å’Œç­”æ¡ˆã€‚")
