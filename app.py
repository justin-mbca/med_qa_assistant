import streamlit as st
import json
import os
import faiss
import numpy as np
import re
from sentence_transformers import SentenceTransformer

# é¡µé¢è®¾ç½®
st.set_page_config(page_title="åŒ»ç–—é—®ç­”åŠ©æ‰‹", page_icon="ğŸ©º")

# æ¸…æ´—æ–‡æœ¬ï¼šå»æ ‡ç‚¹ã€ç©ºç™½ï¼Œç»Ÿä¸€æ ¼å¼
def clean_text(text):
    text = text.strip()
    text = re.sub(r"[ï¼Œ,]", ",", text)
    text = re.sub(r"[ï¼Ÿ?]", "?", text)
    text = re.sub(r"[ï¼!]", "!", text)
    text = re.sub(r"\s+", "", text)
    text = re.sub(r"[,.!?ï¼Ÿï¼ã€‚ï¼Œã€]", "", text)
    return text

# åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜é¿å…é‡å¤åŠ è½½ï¼‰
@st.cache_resource
def load_model():
    return SentenceTransformer("shibing624/text2vec-base-chinese")

model = load_model()

# æ•°æ®è·¯å¾„
KB_PATH = "data/knowledge_base.json"

# åŠ è½½çŸ¥è¯†åº“
def load_knowledge():
    if not os.path.exists(KB_PATH):
        return []
    with open(KB_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

# ä¿å­˜çŸ¥è¯†åº“
def save_knowledge(kb):
    with open(KB_PATH, "w", encoding="utf-8") as f:
        json.dump(kb, f, ensure_ascii=False, indent=2)

# æ„å»ºFAISSç´¢å¼•
def build_index(kb):
    if not kb:
        return None, []
    cleaned_questions = [clean_text(item["question"]) for item in kb]
    embeddings = model.encode(cleaned_questions, convert_to_numpy=True)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index, embeddings

# åŠ è½½çŸ¥è¯†åº“ & æ„å»ºç´¢å¼•
knowledge_base = load_knowledge()
index, embeddings = build_index(knowledge_base)

# æŸ¥è¯¢å‡½æ•°
def search_answer(query, top_k=1):
    if index is None or not knowledge_base:
        return None, 0.0
    cleaned_query = clean_text(query)
    query_vec = model.encode([cleaned_query], convert_to_numpy=True)
    D, I = index.search(query_vec, top_k)
    if D[0][0] < 1.5:  # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
        return knowledge_base[I[0][0]], D[0][0]
    return None, 0.0

# ä¸»é¡µé¢
tab1, tab2 = st.tabs(["ğŸ’¬ é—®ç­”åŠ©æ‰‹", "ğŸ“š æ·»åŠ çŸ¥è¯†"])

with tab1:
    st.title("ğŸ§  åŒ»ç–—é—®ç­”åŠ©æ‰‹")
    st.markdown("è¯·é€‰æ‹©ä¸€ä¸ªæ ·ä¾‹é—®é¢˜ï¼Œæˆ–ç›´æ¥è¾“å…¥æ‚¨çš„å¥åº·é—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸ºæ‚¨æä¾›å»ºè®®ã€‚")

    # é€‰æ‹©æ ·ä¾‹é—®é¢˜ï¼ˆå‰å‡ ä¸ªï¼‰
    sample_questions = [item["question"] for item in knowledge_base[:5]]
    sample_questions.insert(0, "ğŸ”½ è¯·é€‰æ‹©ä¸€ä¸ªæ ·ä¾‹é—®é¢˜")

    selected = st.selectbox("å¸¸è§é—®é¢˜ï¼š", sample_questions, index=0)
    default_query = "" if selected == "ğŸ”½ è¯·é€‰æ‹©ä¸€ä¸ªæ ·ä¾‹é—®é¢˜" else selected

    # æ‰‹åŠ¨è¾“å…¥é—®é¢˜
    query = st.text_input("æˆ–è€…è¾“å…¥å…¶ä»–é—®é¢˜ï¼š", value=default_query, key="query_input")

    if st.button("ğŸ” æŸ¥è¯¢"):
        if not query.strip():
            st.warning("è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜ã€‚")
        else:
            result, score = search_answer(query)
            if result:
                st.success("âœ… æ‰¾åˆ°ç›¸å…³ç­”æ¡ˆï¼š")
                st.markdown(f"**Qï¼š** {result['question']}")
                st.markdown(f"**Aï¼š** {result['answer']}")
                if result.get("tags"):
                    st.caption(f"æ ‡ç­¾ï¼š{'ã€'.join(result['tags'])}")
                st.caption(f"åŒ¹é…ç›¸ä¼¼åº¦åˆ†æ•°ï¼š{1.0 - score:.2f}")
            else:
                st.error("âŒ å¾ˆæŠ±æ­‰ï¼Œå½“å‰çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ç­”æ¡ˆã€‚æ¬¢è¿å½•å…¥æ–°çŸ¥è¯†ï¼")

with tab2:
    st.subheader("ğŸ“¥ æ·»åŠ ä¸“å®¶çŸ¥è¯†")
    new_q = st.text_input("é—®é¢˜ï¼š")
    new_a = st.text_area("ç­”æ¡ˆï¼š")
    new_tags = st.text_input("æ ‡ç­¾ï¼ˆé€—å·åˆ†éš”ï¼Œå¯é€‰ï¼‰")

    if st.button("â• æ·»åŠ "):
        if new_q and new_a:
            new_entry = {
                "question": new_q.strip(),
                "answer": new_a.strip(),
                "tags": [tag.strip() for tag in new_tags.split(",") if tag.strip()]
            }
            knowledge_base.append(new_entry)
            save_knowledge(knowledge_base)
            index, embeddings = build_index(knowledge_base)
            st.success("âœ… æˆåŠŸæ·»åŠ æ–°çŸ¥è¯†ï¼")
        else:
            st.warning("è¯·è¾“å…¥å®Œæ•´çš„é—®é¢˜å’Œç­”æ¡ˆã€‚")
